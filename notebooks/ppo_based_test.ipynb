{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Tianshou's PPO with our CorePolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is mostly meant as a test, and a showcase of how we can combine a CorePolicy with policies provided by Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from environments import Resetting\n",
    "from networks import NetHackObsNet, GoalNetHackActor, GoalNetHackCritic\n",
    "from policies import PPOBasedPolicy\n",
    "from models import SelfModel, EnvModel\n",
    "from intrinsic import ICM\n",
    "from core import GoalCollector, GoalVectorReplayBuffer, GoalOnpolicyTrainer\n",
    "\n",
    "from nle.env.tasks import NetHackGold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Resetting(NetHackGold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 5\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_net = NetHackObsNet(env.observation_space)\n",
    "\n",
    "actor_net = GoalNetHackActor(obs_net, env.action_space)\n",
    "critic_net = GoalNetHackCritic(obs_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buf = GoalVectorReplayBuffer(2000, num_train_envs)\n",
    "test_buf = GoalVectorReplayBuffer(1, num_train_envs)\n",
    "\n",
    "env_model = EnvModel() # this does nothing, for the moment\n",
    "self_model = SelfModel(obs_net, env.action_space, ICM, train_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a single optimizer for actor and critic simplifies the training loop and is more computationally efficient\n",
    "# BUT gradient updates in one network will influence the gradient updates in the other, and this might create unexpected problems...\n",
    "combined_params = set(list(actor_net.parameters()) + list(critic_net.parameters()))\n",
    "optimizer = torch.optim.Adam(combined_params, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PPO-based policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PPOBasedPolicy(\n",
    "    self_model=self_model,\n",
    "    env_model=env_model,\n",
    "    act_net=actor_net, \n",
    "    critic_net=critic_net, \n",
    "    optim=optimizer,\n",
    "    action_space=env.action_space,\n",
    "    observation_space=env.observation_space,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = GoalCollector(policy, train_envs, train_buf)\n",
    "\n",
    "test_collector = GoalCollector(policy, test_envs, test_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_steps_per_epoch = 100\n",
    "\n",
    "step_per_collect = 10\n",
    "episode_per_test = 6\n",
    "batch_size = 10\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"../logs\", \"ppo_based\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GoalOnpolicyTrainer(\n",
    "    policy=policy, \n",
    "    train_collector=train_collector, \n",
    "    test_collector=test_collector,\n",
    "    repeat_per_collect=1,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_stats in trainer:\n",
    "    # TODO a more informative print, plots, logging, etc.\n",
    "    print(epoch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
